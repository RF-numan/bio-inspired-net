# -*- coding: utf-8 -*-
"""BIC update.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iShbnPSIgWGy_af3Xl6ZUiHG3OaRZe-r
"""

import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from IPython.display import clear_output

# Activation Functions
# These define how neurons process inputs at each layer
def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def linear(x):
    return x

# Xavier Initialization for Weights
# Initializes weights for stable training
def xavier_init(input_size, output_size):
    limit = np.sqrt(2 / (input_size + output_size))
    return np.random.uniform(-limit, limit, (input_size, output_size))

# Initialize the ANN (weights and biases for all layers)
def initialize_ANN(layer_sizes):
    weights = []
    biases = []
    for i in range(len(layer_sizes) - 1):
        weights.append(xavier_init(layer_sizes[i], layer_sizes[i + 1]))
        biases.append(np.zeros((1, layer_sizes[i + 1])))
    return weights, biases

# Decode the PSO position into ANN weights and biases
# Converts a flat array into structured weights and biases
def decode_position(position, layer_sizes):
    weights, biases, idx = [], [], 0
    for i in range(len(layer_sizes) - 1):
        weight_size = layer_sizes[i] * layer_sizes[i + 1]
        bias_size = layer_sizes[i + 1]
        weights.append(np.array(position[idx:idx + weight_size]).reshape(layer_sizes[i], layer_sizes[i + 1]))
        idx += weight_size
        biases.append(np.array(position[idx:idx + bias_size]).reshape(1, layer_sizes[i + 1]))
        idx += bias_size
    return weights, biases

# Particle class for PSO
# Represents a single candidate solution in the swarm
class Particle:
    def __init__(self, dimensions, bounds=(-30, 30)):
        self.position = [random.uniform(bounds[0], bounds[1]) for _ in range(dimensions)]
        self.velocity = [random.uniform(-1, 1) for _ in range(dimensions)]
        self.personal_best_position = self.position[:]
        self.personal_best_fitness = float('inf')

# Forward Propagation through the ANN
def feed_forward(X, weights, biases, activation_functions):
    for i in range(len(weights)):
        X = activation_functions[i](np.dot(X, weights[i]) + biases[i])
    return X

# PSO Optimization with Real-Time Visualization
def PSO(layer_sizes, activation_functions, X_train, y_train, X_val, y_val, swarm_size=300, iterations=300, alpha=0.7, beta=1.5, gamma=1.2):
    if len(layer_sizes) - 1 != len(activation_functions):
        raise ValueError("The number of activation functions must match the number of weight layers.")

    # Calculate the total number of parameters (weights + biases)
    dimensions = sum(layer_sizes[i] * layer_sizes[i + 1] for i in range(len(layer_sizes) - 1)) + sum(layer_sizes[1:])
    swarm = [Particle(dimensions) for _ in range(swarm_size)]
    global_best_position = None
    global_best_fitness = float('inf')
    global_best_trend = []

    for iteration in range(iterations):
        for particle in swarm:
            weights, biases = decode_position(particle.position, layer_sizes)
            predictions = feed_forward(X_train, weights, biases, activation_functions)
            fitness = mean_squared_error(y_train, predictions)

            if fitness < particle.personal_best_fitness:
                particle.personal_best_position = particle.position[:]
                particle.personal_best_fitness = fitness

            if fitness < global_best_fitness:
                global_best_position = particle.position[:]
                global_best_fitness = fitness

        for particle in swarm:
            personal_best = np.array(particle.personal_best_position)
            global_best = np.array(global_best_position)
            position = np.array(particle.position)
            velocity = np.array(particle.velocity)

            # Update velocity and position based on PSO equations
            particle.velocity = (
                alpha * velocity +
                beta * (personal_best - position) +
                gamma * (global_best - position)
            )
            particle.position = np.clip(position + particle.velocity, -30, 30)

        global_best_trend.append(global_best_fitness)

        # Real-time Plot Update
        clear_output(wait=True)
        plt.plot(global_best_trend)
        plt.title(f"PSO Optimization Trend (Iteration: {iteration + 1}/{iterations})")
        plt.xlabel("Iteration")
        plt.ylabel("Global Best Fitness (MSE)")
        plt.grid()
        plt.pause(0.1)

        print(f"Iteration {iteration}: Global Best Fitness (MSE): {global_best_fitness}")

        # Early stopping if improvements stall
        if len(global_best_trend) > 20 and np.abs(global_best_trend[-1] - global_best_trend[-20]) < 1e-6:
            print("Early stopping in PSO due to lack of improvement.")
            break

    plt.show()
    return global_best_position, global_best_fitness, global_best_trend

# Main function to combine everything
def main():
    # Load Dataset
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls"
    data = pd.read_excel(url)

    # Preprocess the data (normalize features and split into train/test sets)
    X = data.iloc[:, :-1].values
    y = data.iloc[:, -1].values
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    X = scaler_X.fit_transform(X)
    y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # ANN and PSO configurations
    ann_configurations = [
        {"layers": [X_train.shape[1], 30, 15, 1], "activations": [relu, tanh, linear]},
        {"layers": [X_train.shape[1], 50, 30, 15, 1], "activations": [relu, tanh, sigmoid, linear]},
    ]

    pso_configurations = [
        {"swarm_size": 150, "iterations": 200, "alpha": 0.5, "beta": 1.2, "gamma": 1.0},
        {"swarm_size": 200, "iterations": 300, "alpha": 0.7, "beta": 1.5, "gamma": 1.2},
    ]

    results = []
    for ann_config in ann_configurations:
        for pso_config in pso_configurations:
            best_position, best_fitness, global_best_trend = PSO(
                ann_config["layers"], ann_config["activations"], X_train, y_train, X_val, y_val,
                swarm_size=pso_config["swarm_size"], iterations=pso_config["iterations"],
                alpha=pso_config["alpha"], beta=pso_config["beta"], gamma=pso_config["gamma"]
            )

            weights, biases = decode_position(best_position, ann_config["layers"])
            predictions = feed_forward(X_val, weights, biases, ann_config["activations"])
            mse = mean_squared_error(y_val, predictions)

            results.append({
                "ANN Config": ann_config,
                "PSO Config": pso_config,
                "MSE": mse,
                "Trend": global_best_trend
            })

    # Visualization Enhancements
    best_result = min(results, key=lambda x: x["MSE"])
    plt.figure(figsize=(8, 6))
    plt.plot(best_result["Trend"])
    plt.title(f"Best Optimization Trend: {best_result['ANN Config']['layers']}")
    plt.show()

    plt.figure(figsize=(10, 6))
    mse_values = [r["MSE"] for r in results]
    labels = [f"ANN: {r['ANN Config']['layers']}" for r in results]
    plt.bar(labels, mse_values)
    plt.title("MSE Across Configurations")
    plt.show()

if __name__ == "__main__":
    main()